2024-05-17 07:35:16,816 [INFO ]  Machine name: titan
2024-05-17 07:35:16,816 [INFO ]  Arguments: 
2024-05-17 07:35:16,816 [INFO ]  Namespace(appliance_name='dishwasher', model_arch='cnn', datadir='./dataset_management/my-house', save_dir='/home/lindo/Develop/nilm/ml/models', batchsize=512, n_epoch=50, crop_train_dataset=None, crop_val_dataset=None, plot_display=True)
2024-05-17 07:35:16,816 [INFO ]  Window length: 599
2024-05-17 07:35:16,816 [INFO ]  Training dataset: ./dataset_management/my-house/dishwasher/dishwasher_training_house.csv
2024-05-17 07:35:16,816 [INFO ]  Validation dataset: ./dataset_management/my-house/dishwasher/dishwasher_validation_house.csv
2024-05-17 07:35:16,816 [INFO ]  Restoring model from: /home/lindo/Develop/nilm/ml/models/dishwasher/savemodel_cnn
2024-05-17 07:35:16,816 [INFO ]  Checkpoint file path: /home/lindo/Develop/nilm/ml/models/dishwasher/checkpoints_cnn_fine_tune
2024-05-17 07:35:16,816 [INFO ]  SaveModel file path: /home/lindo/Develop/nilm/ml/models/dishwasher/savemodel_cnn_fine_tune
2024-05-17 07:35:16,816 [INFO ]  Training history file path: /home/lindo/Develop/nilm/ml/models/dishwasher/history_cnn_fine_tune
2024-05-17 07:35:16,975 [INFO ]  There are 0.533M training samples.
2024-05-17 07:35:16,975 [INFO ]  There are 0.164M validation samples.
2024-05-17 07:35:16,975 [INFO ]  Normalized on power threshold: 0.004
2024-05-17 07:35:16,975 [INFO ]  L1 loss multiplier: 1.0
2024-05-17 07:35:16,975 [INFO ]  Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')
2024-05-17 07:35:17,255 [INFO ]  Number of replicas: 2.
2024-05-17 07:35:17,256 [INFO ]  Global batch size: 1024.
2024-05-17 07:35:17,306 [INFO ]  Learning rate: 0.0001
2024-05-17 07:35:17,688 [WARNI]  No training configuration found in save file, so the model was *not* compiled. Compile it manually.
2024-05-17 07:35:26,299 [INFO ]  Collective all_reduce tensors: 6 all_reduces, num_devices = 2, group_size = 2, implementation = CommunicationImplementation.NCCL, num_packs = 1
2024-05-17 07:35:27,836 [INFO ]  Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2024-05-17 07:35:28,693 [INFO ]  Collective all_reduce tensors: 6 all_reduces, num_devices = 2, group_size = 2, implementation = CommunicationImplementation.NCCL, num_packs = 1
2024-05-17 07:35:28,814 [INFO ]  Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2024-05-17 07:35:32,234 [INFO ]  Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2024-05-17 07:35:32,236 [INFO ]  Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2024-05-17 07:35:32,239 [INFO ]  Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2024-05-17 07:35:32,239 [INFO ]  Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2024-05-17 07:35:32,258 [INFO ]  Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2024-05-17 07:35:32,259 [INFO ]  Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2024-05-17 07:35:32,259 [INFO ]  Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2024-05-17 07:35:32,260 [INFO ]  Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
2024-05-17 07:35:44,143 [INFO ]  Running test loop after epoch: 1.
2024-05-17 07:35:46,551 [INFO ]  Current val loss of 0.2493 < than val loss of inf, saving model to /home/lindo/Develop/nilm/ml/models/dishwasher/savemodel_cnn_fine_tune.
2024-05-17 07:35:46,551 [WARNI]  Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.
2024-05-17 07:35:47,636 [INFO ]  Assets written to: /home/lindo/Develop/nilm/ml/models/dishwasher/savemodel_cnn_fine_tune/assets
2024-05-17 07:35:47,650 [INFO ]  epoch: 1 loss: 0.3791 mse: 0.002226 mae: 0.03009 val_loss: 0.2493 val_mse: 0.000915 val_mae: 0.01221
2024-05-17 07:35:47,900 [INFO ]  Reshuffling training dataset.
2024-05-17 07:35:59,961 [INFO ]  Running test loop after epoch: 2.
2024-05-17 07:36:01,387 [INFO ]  epoch: 2 loss: 0.2225 mse: 0.000509 mae: 0.01285 val_loss: 0.2816 val_mse: 0.0005437 val_mae: 0.006359
2024-05-17 07:36:01,655 [INFO ]  Reshuffling training dataset.
2024-05-17 07:36:13,636 [INFO ]  Running test loop after epoch: 3.
2024-05-17 07:36:15,010 [INFO ]  epoch: 3 loss: 0.2012 mse: 0.0003279 mae: 0.009108 val_loss: 0.2942 val_mse: 0.0006027 val_mae: 0.006202
2024-05-17 07:36:15,301 [INFO ]  Reshuffling training dataset.
2024-05-17 07:36:27,520 [INFO ]  Running test loop after epoch: 4.
2024-05-17 07:36:28,950 [INFO ]  epoch: 4 loss: 0.1874 mse: 0.0002593 mae: 0.007405 val_loss: 0.2764 val_mse: 0.0004886 val_mae: 0.005457
2024-05-17 07:36:29,243 [INFO ]  Reshuffling training dataset.
2024-05-17 07:36:41,417 [INFO ]  Running test loop after epoch: 5.
2024-05-17 07:36:42,870 [INFO ]  epoch: 5 loss: 0.1797 mse: 0.0002187 mae: 0.006325 val_loss: 0.2801 val_mse: 0.0004198 val_mae: 0.004386
2024-05-17 07:36:43,163 [INFO ]  Reshuffling training dataset.
2024-05-17 07:36:55,238 [INFO ]  Running test loop after epoch: 6.
2024-05-17 07:36:56,654 [INFO ]  epoch: 6 loss: 0.1738 mse: 0.0002001 mae: 0.005755 val_loss: 0.2721 val_mse: 0.0004537 val_mae: 0.003495
2024-05-17 07:36:56,948 [INFO ]  Reshuffling training dataset.
2024-05-17 07:37:09,082 [INFO ]  Running test loop after epoch: 7.
2024-05-17 07:37:10,530 [INFO ]  epoch: 7 loss: 0.1721 mse: 0.0001708 mae: 0.005134 val_loss: 0.2835 val_mse: 0.0003819 val_mae: 0.003827
2024-05-17 07:37:10,820 [INFO ]  Early termination of training.
